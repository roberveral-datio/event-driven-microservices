# Schema

The main problem with Kafka is that it stores serialized messages, as bytes, so it's easy to try 
to read a String as Long and obtain a deserialization exception.

To avoid this, it's important to manage the types that are stored in a Kafka topic, so producers 
and consumers are aware of the type of messages they could expect when reading from a topic.

**Avro** is the most common format used in Kafka, because of the following properties:
- It supports **schema evolution** (you can evolve the schema without breaking old consumers).
- It's supported by the **Schema Registry**, which helps to ensure that only compatible messages are stored in a given topic.
- It han built-in documentation for the fields, using the **"docs" attribute**.

Managing schemas in its own repository/module offers a lot of advantages in terms of government:
- Strict review of PRs to include a new type or to modify a previous one, checking the compatibility and that it's well documented.
- The events are the "glue" between microservices. Having it in a centralized location helps that every service has the same definitions.
- We only store Avro files, but they're compiled into autogenerated Java classes. You can then add the dependency in your service.

The schemas created for the example are the following:
- **UserCreated**: event which happens when a user is registered.
- **UserNotified**: event which happens when a user notification is sent.
